{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from opentldr import Workflow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Repo Setup\n",
                "\n",
                "This should be modified based on where the data is located.\n",
                "This can could alternatively return an S3 bucket config."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_repo_prefix = '../Data/Sample'\n",
                "date_string = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_data_repo_config(folder_name:str) -> dict:\n",
                "        '''\n",
                "        builds a simple config for files. Folder_name is inserted into path.\n",
                "        (for example \"content\")\n",
                "        if a date_string is specified, it will include that.\n",
                "        '''\n",
                "        if date_string is None:\n",
                "                return {\n",
                "                        'repo_type': 'files', \n",
                "                        'path': '{}/{}'.format(data_repo_prefix,folder_name),\n",
                "                }\n",
                "        else:\n",
                "                return {\n",
                "                        'repo_type': 'files', \n",
                "                        'path': '{}/{}/{}'.format(data_repo_prefix, date_string, folder_name),\n",
                "                }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LLM Setup\n",
                "\n",
                "This should be modified to output an LLM config that you want to use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "type = 'ollama'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_llm_config() -> dict:\n",
                "        '''\n",
                "        builds a simple llm config that inserts model info.\n",
                "        '''\n",
                "        match (type.lower()):\n",
                "            case \"ollama\":\n",
                "                return {\n",
                "                    'type': 'Ollama',\n",
                "                    'device':'local',\n",
                "                    'model':'mistral:latest'\n",
                "                }\n",
                "            case \"gpt4all\":\n",
                "                return {\n",
                "                    'type': 'GPT4ALL',\n",
                "                    'device':'gpu',\n",
                "                    'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'\n",
                "                }\n",
                "            case _ :\n",
                "                return {\n",
                "                    'type': 'GPT4ALL',\n",
                "                    'device':'gpu',\n",
                "                    'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'\n",
                "                }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Specify the Workflow in code\n",
                "The workflow includes:\n",
                "- **Output**: the directory that the workflow writes copies of the notebooks as executed (read only!)\n",
                "- **Notebooks**: this is a list of notebooks (full path) in the order that they should be executed\n",
                "    - For each notebook the set of parameters that are to be passed into it thru the workflow process"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "workflow = {\n",
                "    \n",
                "    # Where a read-only version of the notebook AFTER execution is stored\n",
                "    \"Output\": \"./READ_ONLY_OUTPUT\",\n",
                "    \n",
                "    # Parameters passed into all notebooks in workflow\n",
                "    \"Common\": {\n",
                "        \"logging_level\":10,\n",
                "        \"verbose\": True,\n",
                "    },\n",
                "\n",
                "    # Order and parameters of notebooks to execute in workflow\n",
                "    \"Notebooks\": [\n",
                "\n",
                "        # Setup the KG\n",
                "        [ \"Stage_1_Initialize/Clear_All.ipynb\", {\n",
                "            \"message\": \"Successfully passed in parameters from Workflow.ipynb!\",\n",
                "        }],\n",
                "\n",
                "        [ \"Stage_1_Initialize/Load_Reference_Data.ipynb\", {\n",
                "            \"data_repo_config\": build_data_repo_config('reference'),\n",
                "        }],\n",
                "\n",
                "        # Load Content and Requests\n",
                "        [ \"Stage_2_Ingest/Load_Content.ipynb\", {\n",
                "            \"data_repo_config\": build_data_repo_config('content'),\n",
                "        }],\n",
                "\n",
                "        [ \"Stage_2_Ingest/Load_Requests.ipynb\", {\n",
                "            \"data_repo_config\": build_data_repo_config('request'),\n",
                "        }],\n",
                "\n",
                "        # Generate stand-alone untailored summaries ONLY NEEDED for multi-doc comparisons in digger\n",
                "        [ \"Stage_5_Summarize/Presummarize.ipynb\",{\n",
                "            \"llm_config\" : build_llm_config(),\n",
                "        }],\n",
                "\n",
                "        # Perform Analytics to link entities in Requests and Content nodes\n",
                "        [ \"Stage_3_Connect/Entity_Cosin_Similarity.ipynb\",{\n",
                "            \"sentence_embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
                "            \"connect_threshold\": 0.6,\n",
                "            \"hypothesize_threshold\": 0.9\n",
                "        }],\n",
                "\n",
                "        # Compute recommendations based on relevance of content to request\n",
                "        [ \"Stage_4_Recommend/Shortest_Path_Scoring.ipynb\", {\n",
                "            \"recommendation_threshold\": 0.6\n",
                "        }],\n",
                "\n",
                "        # Generate a summary of the content that is tailored with respect to the request and useful reference knowledge\n",
                "        [ \"Stage_5_Summarize/Tailored_Abstractive_Summary.ipynb\", {\n",
                "            \"llm_config\" : build_llm_config(),\n",
                "            \"llm_prompt\": \"You are a helpful assistant responding to the request: {request} \\n\\n and were given these facts: {knowledge} \\n\\n Concisely summarize the following article: {content}\"\n",
                "        }],\n",
                "\n",
                "        # Produce a TLDR Report for each request\n",
                "        [ \"Stage_6_Produce/Build_TLDR.ipynb\", {}],\n",
                "\n",
                "        # Run the Evaluation\n",
                "        [ \"Stage_7_Evaluate/Evaluate.ipynb\", {\n",
                "            \"data_repo_config\": build_data_repo_config('evalkey'),\n",
                "            \"sentence_embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "        }]\n",
                "    ]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wf:Workflow = Workflow(workflow)\n",
                "wf.run()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}