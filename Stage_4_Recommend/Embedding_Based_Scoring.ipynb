{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b29b70f4-f7b6-48bb-ac19-a75a58504fbc",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Stage 4: Recommendations using an Embedding Model to Compute Relevance Between Requests and Articles\n",
                "Attempts to determine how relevent each Content node is to each Request node and builds Recommendation nodes to store that score for highly rated relevance.\n",
                "\n",
                "The result of this step includes:\n",
                "- Recommendation nodes, connected to Content nodes with a RECOMMENDS, and Request nodes with a RELATES_TO relationship"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "556dd7c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "from sentence_transformers import SentenceTransformer, util\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "967df770",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "OpenTLDR workflows use the notebook block tagged as \"parameters\" to inject variables (for example to change the recommendation thresholds).\n",
                "\n",
                "> **Do Not Change Variable Names in the Parameters Block** you are welcome to change the values of these parameter variables, but please do not change their names. They are used elsewhere in the notebook and in other workflow processes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0567be2e",
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "#Parameters\n",
                "\n",
                "# configuraiton of embedding model\n",
                "sentence_embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "embedding_similarity_alg = \"dot_product\"\n",
                "#embedding_similarity_alg = \"cosine_sim\"\n",
                "\n",
                "# configuration for general filtering\n",
                "recommendation_threshold = 0.60\n",
                "recommendation_topk = 5\n",
                "use_click_prediction = False\n",
                "\n",
                "# only set to true if you are going to recompute all the recommedations (e.g. debugging)\n",
                "delete_existing_recommendations = True\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level = logging.INFO\n",
                "\n",
                "# List of the Requests to process recommendations for\n",
                "list_of_uids = None\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cde082e8",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "### Connect to Knowledge Graph (where Requests, Users, and Content reside)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "96e7cb0d",
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.getLogger(\"OpenTLDR\").setLevel(logging_level)\n",
                "\n",
                "import opentldr.Domain as domain\n",
                "from opentldr.Domain import Request, Content, Recommendation, User, Feedback, TldrEntry\n",
                "\n",
                "from opentldr import KnowledgeGraph\n",
                "kg=KnowledgeGraph()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c58b7452",
            "metadata": {},
            "source": [
                "### Determine which Requests we need to process (defaults to all)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "659dcb3e",
            "metadata": {},
            "outputs": [],
            "source": [
                "if list_of_uids is None:\n",
                "    list_of_uids = kg.get_all_node_uids_by_tag('Request')\n",
                "\n",
                "if verbose:\n",
                "    print(\"Found {} Request nodes to process.\".format(len(list_of_uids)))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1c085fa",
            "metadata": {},
            "source": [
                "### Check if we should remove existing recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8fad3452",
            "metadata": {},
            "outputs": [],
            "source": [
                "if delete_existing_recommendations:\n",
                "    kg.delete_all_recommendations()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e27524b0",
            "metadata": {},
            "source": [
                "# Candidate Generation\n",
                "- This notebook follows the \"Two Towers\" model for candate generation.\n",
                "- It uses the same Text Embedding Model for each \"Tower\".\n",
                "- The similarity (either 'cosine_sim' or 'dot_product') compares the embeddings.\n",
                "- If the similarity score is greater than or equal to the provided threshold, the item becomes a candidate.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "206250f1",
            "metadata": {},
            "source": [
                "## Embedding Model(s)\n",
                "In this case, the same simple SBERT/Sentance Transformer model for both the Content and Request text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e858031",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SentenceTransformer(sentence_embedding_model)\n",
                "\n",
                "# Tower One\n",
                "# Items in OpenTLDR are Content nodes - they have text content (and other things) that can be used to recommend\n",
                "\n",
                "content_embedding_cache = {}    # don't recompute the same content embeddings\n",
                "\n",
                "def item_embedding(content:Content):\n",
                "        if content.uid in content_embedding_cache:\n",
                "                return content_embedding_cache[content.uid]\n",
                "        else:\n",
                "                tensor = model.encode(content.title+\"\\n\"+content.text, convert_to_tensor=True)\n",
                "                content_embedding_cache[content.uid] = tensor\n",
                "                return tensor\n",
                "\n",
                "# Tower Two\n",
                "# We don't use \"user\" because the \"request\" seperates interests, and users can have multiple very different requests.\n",
                "\n",
                "def user_embedding(user:User, request:Request):\n",
                "        return model.encode(request.text, convert_to_tensor=True)\n",
                "\n",
                "\n",
                "# Another, hey wait, that's Three Towers...\n",
                "# this is used to get embeddings of summaries that they user has previously\n",
                "# clicked on in TLDRs based on this request. So, a click history...\n",
                "def history_embedding(feedback:Feedback):\n",
                "        entry:TldrEntry = feedback.about_entry.single()\n",
                "        text = entry.title+\"\\n\"+entry.summary\n",
                "        return model.encode(text, convert_to_tensor=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f92705d",
            "metadata": {},
            "source": [
                "## Similarity Score\n",
                "\n",
                "Options supported here (by setting embedding_similarity_alg) are dot_product and cosine_sim."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4cf6c84c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def similarity(embedding_1, embedding_2) -> float:\n",
                "\n",
                "        #compute the cosin similarity of the two embeddings\n",
                "        match (embedding_similarity_alg.lower()):\n",
                "            case ('dot_product'):\n",
                "                return util.dot_score(embedding_1, embedding_2).cpu().numpy()[0][0]\n",
                "            case ('cosine_sim'):\n",
                "                return util.cos_sim(embedding_1, embedding_2).cpu().numpy()[0][0]\n",
                "             \n",
                "            case _:\n",
                "                logging.warning(f\"No embedding similarity function found for {embedding_similarity_alg}, returning 0.0.\")\n",
                "                return 0.0"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b703cea8",
            "metadata": {},
            "source": [
                "## Candidate Filtering\n",
                "- Using Similarity to determine if a combination is a candidate or not.\n",
                "- In this case, we just compare it to the threshold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1eab2e12",
            "metadata": {},
            "outputs": [],
            "source": [
                "def is_candidate (user:User, request:Request, content:Content, similarity_score:int) -> bool:\n",
                "    return similarity_score >= recommendation_threshold"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b987dc7",
            "metadata": {},
            "source": [
                "# Ranking\n",
                "- It is not clear that any Top-K filter makes sense for the TLDR problem, so you can set it to -1 to allow everything.\n",
                "- Ranking is simply a sorting by the scores previously computed, more complicated approaches might do better.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c851e36",
            "metadata": {},
            "outputs": [],
            "source": [
                "def rank (content_scores:dict, topk:int = -1) -> list:\n",
                "    \n",
                "    out= sorted(content_scores.items(), key=lambda x:x[1], reverse=True)\n",
                "\n",
                "    if topk == -1:\n",
                "        return out\n",
                "    else:\n",
                "        return out[:topk]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81219938",
            "metadata": {},
            "source": [
                "### Recalibrate Scores\n",
                "\n",
                "- history_recalibrate_score: If there is feedback that adjusted a previous recommendation score, and\n",
                "this node was alot like that one. Try to apply a similar adjustment to this score as well.\n",
                "\n",
                "- simple_recalibrate_score: Give it a little buff since they are usually so low."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8baf7eaf",
            "metadata": {},
            "outputs": [],
            "source": [
                "def history_recalibrate_score (feedback:Feedback, score:float) -> float:\n",
                "    if feedback.score == -1:\n",
                "        return score\n",
                "    \n",
                "    # adjusts this score similar to how a user adjusted similar score.\n",
                "    entry:TldrEntry = feedback.about_entry.single()\n",
                "    diff = feedback.score - entry.score\n",
                "    print (diff)\n",
                "    out = score + diff\n",
                "\n",
                "    # ensure we stay in range\n",
                "    if out > 1.0:\n",
                "        out = 1.0\n",
                "    \n",
                "    if out < 0.0:\n",
                "        out = 0.0\n",
                "\n",
                "    return out\n",
                "\n",
                "def simple_recalibrate_score (score:float) -> float:\n",
                "    # embedding simiarity tends to low, this buffs it a bit\n",
                "    return (score +1.0) / 2.0\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2075c130",
            "metadata": {},
            "source": [
                "### Average the distance relevance scores for each Article based on its neighbors "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "5950fae0",
            "metadata": {},
            "source": [
                "# Process Each Query in the System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ebbf4836",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Items to process (all)\n",
                "content_uids = kg.get_all_node_uids_by_tag('Content')\n",
                "\n",
                "# Loop thru Requests\n",
                "for request_uid in list_of_uids:\n",
                "    request = kg.get_request_by_uid(request_uid)\n",
                "    user = request.get_requested_by()\n",
                "\n",
                "    print(f\"\\nRequest: {request.title} by {user.name}\")\n",
                "    request_embedding = user_embedding(user,request)\n",
                "\n",
                "    # Get any user clicked or rated content for this request\n",
                "    feedback_nodes = kg.get_feedback_by_request(request)\n",
                "\n",
                "    # Perform Candidate Generation\n",
                "    content_scores=dict()\n",
                "\n",
                "    # Loop thru items and score them\n",
                "    for content_uid in content_uids:\n",
                "        content = kg.get_content_by_uid(content_uid)\n",
                "        content_embedding = item_embedding(content)\n",
                "\n",
                "        similarity_score = similarity(request_embedding, content_embedding)\n",
                "        recommendation_score:float = simple_recalibrate_score(similarity_score)\n",
                "\n",
                "        # Click Prediction - is this content more similar to other content\n",
                "        # previously clicked on for this request?\n",
                "        if use_click_prediction:\n",
                "            for feedback in feedback_nodes:\n",
                "\n",
                "                tldr_entry:TldrEntry = feedback.about_entry.single()\n",
                "                print (tldr_entry.title)\n",
                "\n",
                "                # feedback.click_date is set if the user clicked thru to source content\n",
                "                if feedback.click_date is not None:\n",
                "                    feedback_embedding = history_embedding(feedback)\n",
                "                    feedback_similarity = simple_recalibrate_score(similarity(feedback_embedding, content_embedding))\n",
                "                    # take the best score and use that\n",
                "                    if feedback_similarity > recommendation_score:\n",
                "                        recommendation_score = feedback_similarity\n",
                "\n",
                "        if is_candidate(user, request, content, recommendation_score):\n",
                "            content_scores[content_uid]= recommendation_score \n",
                "            if verbose:\n",
                "                print(\"\\tIS a candidate:\\t {} ({})\".format(content.title, str(recommendation_score)))\n",
                "        else:\n",
                "            if verbose:\n",
                "                print(\"\\tNOT a candidate:\\t {} ({})\".format(content.title, str(recommendation_score)))\n",
                "\n",
                "    # Assumption is that there are too many Content nodes (with text) to cache the objects in memory.\n",
                "    if verbose:\n",
                "        print(\"\\nRanking:\")\n",
                "\n",
                "    feedback_nodes = kg.get_feedback_by_request(request)\n",
                "\n",
                "    # Rank the item candidates\n",
                "    for item in rank(content_scores, recommendation_topk):\n",
                "        content = kg.get_content_by_uid(item[0])\n",
                "        recommendation_score = item[1]\n",
                "        print(\"\\tRecommending:\\t {} ({})\".format(content.title, str(recommendation_score)))\n",
                "\n",
                "        # Ultimately THIS is the call any recommender needs to make for each content node (i.e., item) recommended.\n",
                "        kg.add_recommendation(request=request,content=content,score=recommendation_score)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "902fe103-5cd3-454b-8084-f6b2b6ff92b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "kg.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}