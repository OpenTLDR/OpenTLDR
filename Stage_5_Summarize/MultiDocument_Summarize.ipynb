{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Summarize Multiple Documents\n",
                "Uses the recommendations and related paths to focus an LLM summerization on the relevant information.\n",
                "\n",
                "This notebook depends on the Content nodes already having summaries included in metadata.\n",
                "This is performed by either the \"Abstractive Summary\" notebook or the \n",
                "\n",
                "The result of this step includes:\n",
                "- Summary nodes, connected to Content nodes with a SUMMARIZES relationship and to Recommendation nodes with a FOCUS_ON relationship"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "import dotenv\n",
                "from Summarizer import getSummarizer\n",
                "\n",
                "dotenv.load_dotenv()  # Load environment variables from .env file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "OpenTLDR workflows use the notebook block tagged as \"parameters\" to inject variables (for example to change the LLM model).\n",
                "\n",
                "> **Do Not Change Variable Names in the Parameters Block** you are welcome to change the values of these parameter variables, but please do not change their names. They are used elsewhere in the notebook and in other workflow processes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "#Parameters\n",
                "\n",
                "# If you set the llm_config to None, it will use the environment variable LLM_CONFIG\n",
                "# Otherwise, here are some options (to run an LLM locally, you will need to download the model to your local machine)\n",
                "# llm_config = {'type': 'GPT4ALL', 'device':'gpu', 'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'}\n",
                "# llm_config = {'type': 'Ollama', 'model':'mistral:latest'}\n",
                "# llm_config = {'type': 'ChatGPT', 'model':'gpt-4'}\n",
                "llm_config = None\n",
                "\n",
                "llm_prompt = '''\n",
                "        How do the following articles address: \"{request}\"?\n",
                "        In your response, first identify what the articles have in common.\n",
                "        Second, your response should include highlights of the main differences between the articles.\n",
                "        The articles below are preceded by their title and separated from the other summaries using '========'. \\n\n",
                "        Summarize the following articles:\\n {multicontent}\n",
                "    '''\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level = logging.INFO\n",
                "\n",
                "# List of the UniqueIds to Ingest\n",
                "list_of_uids = None\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from opentldr import KnowledgeGraph\n",
                "import opentldr.Domain as domain\n",
                "\n",
                "logging.getLogger(\"OpenTLDR\").setLevel(logging_level)\n",
                "\n",
                "kg=KnowledgeGraph()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if list_of_uids is None:\n",
                "    list_of_uids = kg.get_all_node_uids_by_tag(\"Request\")\n",
                "\n",
                "if verbose:\n",
                "    print (\"Found {} Request nodes to attempt multi-document summarization.\".format(len(list_of_uids)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run an LLM Model\n",
                "This notebook uses the `Summarizer` class to run an LLM model. \n",
                "You can set the LLM model by changing the `llm_config` variable in the parameters block above or setting LLM_CONFIG in the .env file or environment variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import Summarizer\n",
                "llm:Summarizer = Summarizer.getSummarizer(llm_config, logging_level=logging_level)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build the prompt and run the LLM\n",
                "This includes the original content, the request it is tailored for, and the explaination of the shortest path through the knowledge graph used to connect them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_group_summary(kg:KnowledgeGraph, request:domain.Request) -> str:\n",
                "    len_original=0\n",
                "    len_summaries=0\n",
                "    multisum=\"\"\n",
                "    multidoc_summary=\"Sorry - No Group Summary Created.\"\n",
                "    print(\"summarizing recommendations for request {}\".format(request.title))\n",
                "\n",
                "    recommendations = kg.get_recommendations_by_request(request)\n",
                "    # in the case there is only one, just sent that summary instead\n",
                "    if len(recommendations) == 1:\n",
                "        return \"Only one recommendation.\"\n",
                "\n",
                "    for recommendation in recommendations:\n",
                "        for content in recommendation.recommends:\n",
                "            print (\"Recommended {title} ({score}): {url}\".format(title=content.title, score=round(recommendation.score,3),url=content.url))\n",
                "            len_original += len(content.text)\n",
                "\n",
                "            if content.metadata is not None:\n",
                "                summary=content.metadata[\"summary\"]\n",
                "                len_summaries += len(summary)\n",
                "\n",
                "                if len(summary) > 0:\n",
                "                    multisum+=\"\\n\\n========n\\n\\nArticle Title: {}:\\n\".format(content.title)\n",
                "                    multisum+=\"{}\\n\".format(summary)\n",
                "\n",
                "            else:\n",
                "                print(\"skipping - no metadata summary found\")\n",
                "\n",
                "    if len(multisum) > 0:\n",
                "        prompt_text = llm_prompt.format(multicontent=multisum, request=request.text)\n",
                "        multidoc_summary= llm.summarize(prompt_text)\n",
                "        \n",
                "        print(\"\\nSummary reduced {reduction}% of content:\\t{text}\".format(reduction=round(((len_original-len(multidoc_summary))/len_original)*100,1),text=multidoc_summary))\n",
                "\n",
                "    return multidoc_summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for uid in list_of_uids:\n",
                "    request = kg.get_request_by_uid(uid)\n",
                "\n",
                "    summary = run_group_summary(kg, request)\n",
                "\n",
                "    tldr = kg.get_tldr_by_request(request)\n",
                "\n",
                "    if tldr is not None:\n",
                "        tldr.metadata[\"summary\"]= summary\n",
                "        tldr.save()\n",
                "    else:\n",
                "        print(summary)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kg.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}