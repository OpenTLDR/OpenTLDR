{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Tailored Abstractive Summary\n",
                "A \"summary\" is a shorted restatement of the information from the target content, ideally it maintains information with fewer words.\n",
                "\n",
                "An \"abstractive\" summary is a shorter restatement of the information from content without any requirement to reuse the same words or phrasing. This is often implemented using a generative approach. This is usually compared to an \"extractive\" summary which reduces the content word count by selecting words and phrases from the original content and removing everything else.\n",
                "\n",
                "A \"tailored\" summary is a summary that prioritizes specific information during the summarization. In this case, we attempt to focus on the information requested by the user (in the User's Request statement). Here, we also augment the abstractive summary process with curated reference knowledge that makes the connection between the request and the content that justified the recommendation in the first place. Tailoring is often implemented like RAG by augmenting the prompt sent to a large language model with information we expect to be useful - unlike traditional RAG, Tailoring does attempts to direct the generation rather then add additional information.\n",
                "\n",
                "The result of this step includes:\n",
                "- Summary nodes, connected to Content nodes with a SUMMARIZES relationship and to Recommendation nodes with a FOCUS_ON relationship"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "OpenTLDR workflows use the notebook block tagged as \"parameters\" to inject variables (for example to change the LLM model).\n",
                "\n",
                "> **Do Not Change Variable Names in the Parameters Block** you are welcome to change the values of these parameter variables, but please do not change their names. They are used elsewhere in the notebook and in other workflow processes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "#Parameters\n",
                "\n",
                "# When run an LLM locally, you need to download the model to your local machine\n",
                "llm_config = {'type': 'GPT4ALL', 'device':'gpu', 'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'}\n",
                "#llm_config = {'type': 'Ollama', 'device':'local', 'model':'mistral:latest'}\n",
                "\n",
                "llm_prompt = '''\n",
                "    Given these facts: {knowledge}\n",
                "    Concisely summarize this content: {content}\n",
                "    While focusing on answering this: {request}\n",
                "    '''\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level= logging.INFO\n",
                "\n",
                "# List of the UniqueIds of Requests to add summaries\n",
                "list_of_uids = None\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.getLogger(\"OpenTLDR\").setLevel(logging_level)\n",
                "\n",
                "from opentldr import KnowledgeGraph\n",
                "kg=KnowledgeGraph()\n",
                "\n",
                "import opentldr.Domain as domain"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if list_of_uids is None:\n",
                "    # default to getting all Requests\n",
                "    list_of_uids = kg.get_all_node_uids_by_tag(\"Request\")\n",
                "\n",
                "if verbose:\n",
                "    print (\"Found {} Request nodes to summarize.\".format(len(list_of_uids)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run an LLM Model\n",
                "This cell setups of access to a (usually locally running) LLM based on the llm_config parameter.\n",
                "\n",
                "Ollama: runs locally with the Ollama service\n",
                "- You need to start the Ollama server (ollama serve)\n",
                "- It will attempt to pull models based on config\n",
                "\n",
                "GPT4ALL: runs locally with a .gguf formatted model.\n",
                "- When you run an LLM localling using GPT4ALL, you need to download a model file to your local machine.\n",
                "- Model files are large and not part of the git repository.\n",
                "- You can download them from here: https://gpt4all.io under \"Model Explorer\" and put them in a \"models\" folder.\n",
                "\n",
                "All:\n",
                "- Be sure to check the license for the model before using.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from SummarizeWithGPT4All import SummarizeWithGPT4All\n",
                "from SummarizeWithLocalOllama import SummarizeWithLocalOllama\n",
                "\n",
                "llm = None\n",
                "\n",
                "match (llm_config['type'].lower()):\n",
                "    case \"gpt4all\": \n",
                "        llm = SummarizeWithGPT4All(llm_config['model'],device=llm_config['device'], logging_level=logging_level)\n",
                "\n",
                "    case \"ollama\":\n",
                "        # TODO config for local and remote ollama services\n",
                "        llm = SummarizeWithLocalOllama(model_name=llm_config['model'], logging_level=logging_level)\n",
                "    case _:\n",
                "        raise ValueError(\"No LLM type support for {}.\".format(llm_config['type']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compute the Shortest Path for each Recommendation between the Source Article and Query (excluding the recommendation itself)\n",
                "- TODO: processing the path could be much more interesting that it is now but doing more with other nodes/edges"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def explain(something):\n",
                "    if hasattr(something, 'to_text') and callable(something.to_text):\n",
                "        return something.to_text()\n",
                "    \n",
                "    return \"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build the prompts and run the LLM\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for request_uid in list_of_uids:\n",
                "    request = kg.get_request_by_uid(request_uid)\n",
                "\n",
                "    if request is None:\n",
                "        print(\"No Request found for uid: {}\".format(request_uid))\n",
                "\n",
                "    print(\"Request ({request}):\".format(request=request.text))\n",
                "    recommendations = kg.get_recommendations_by_request(request=request)\n",
                "    \n",
                "    for recommendation in recommendations:\n",
                "        for content in recommendation.recommends:\n",
                "            print (\"Recommended {title} ({score}): {url}\".format(title=content.title, score=round(recommendation.score,3),url=content.url))\n",
                "            path_text=\"\"\n",
                "            path=kg.shortest_path(request,content)\n",
                "\n",
                "            if path is not None:\n",
                "                for hop in path:\n",
                "                    path_text+=explain(hop)+\" \"\n",
                "            \n",
                "            original= content.text\n",
                "            if verbose:\n",
                "                print(\"\\tOriginal Content:\\t{text}\".format(text=original))\n",
                "                print(\"\\tPath Text:\\t{text}\".format(text=path_text))\n",
                "\n",
                "            prompt_text = llm_prompt.format(knowledge=path_text, content=content.text, request=request.text )\n",
                "            summary = llm.summarize(prompt_text)\n",
                "            kg.add_summary(text=summary,content=content,recommendation=recommendation)\n",
                "\n",
                "            print(\"Summary reduced {reduction}% of content:\\t{text}\\n\".format(reduction=round(((len(original)-len(summary))/len(original))*100,1),text=summary))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kg.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}