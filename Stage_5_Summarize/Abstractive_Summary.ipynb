{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Abstractive Summary\n",
                "A \"summary\" is a shorted restatement of the information from the target content, ideally it maintains information with fewer words.\n",
                "\n",
                "An \"abstractive\" summary is a shorter restatement of the information from content without any requirement to reuse the same words or phrasing. This is often implemented using a generative approach. This is usually compared to an \"extractive\" summary which reduces the content word count by selecting words and phrases from the original content and removing everything else.\n",
                "\n",
                "Note: this notebook does NOT implement \"tailoring\" of the summarization.\n",
                "\n",
                "The result of this step includes:\n",
                "- Summary nodes, connected to Content nodes with a SUMMARIZES relationship and to Recommendation nodes with a FOCUS_ON relationship"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "import json\n",
                "import dotenv\n",
                "\n",
                "dotenv.load_dotenv()  # Load environment variables from .env file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "OpenTLDR workflows use the notebook block tagged as \"parameters\" to inject variables (for example to change the LLM model).\n",
                "\n",
                "> **Do Not Change Variable Names in the Parameters Block** you are welcome to change the values of these parameter variables, but please do not change their names. They are used elsewhere in the notebook and in other workflow processes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "#Parameters\n",
                "\n",
                "# If you set the llm_config to None, it will use the environment variable LLM_CONFIG\n",
                "# Otherwise, here are some options (to run an LLM locally, you will need to download the model to your local machine)\n",
                "# llm_config = {'type': 'GPT4ALL', 'device':'gpu', 'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'}\n",
                "# llm_config = {'type': 'Ollama', 'model':'mistral:latest'}\n",
                "# llm_config = {'type': 'ChatGPT', 'model':'gpt-4'}\n",
                "llm_config = None\n",
                "\n",
                "llm_prompt = '''\n",
                "    Summarize this content: {content}\n",
                "    '''\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level = logging.INFO\n",
                "\n",
                "# List of the UniqueIds to Ingest\n",
                "list_of_uids = None\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.getLogger(\"OpenTLDR\").setLevel(logging_level)\n",
                "\n",
                "import opentldr.Domain as domain\n",
                "from opentldr import KnowledgeGraph\n",
                "\n",
                "kg=KnowledgeGraph()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if list_of_uids is None:\n",
                "    # default to getting all not-previously summarized Content nodes\n",
                "    list_of_uids = kg.cypher_query(\"MATCH (n:Content) WHERE NOT (n)<-[:SUMMARIZES]-() RETURN n.uid\")\n",
                "\n",
                "if verbose:\n",
                "    print (\"Found {} Content nodes without Summaries\".format(len(list_of_uids)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run an LLM Model\n",
                "This notebook uses the `Summarizer` class to run an LLM model. \n",
                "You can set the LLM model by changing the `llm_config` variable in the parameters block above or setting LLM_CONFIG in the .env file or environment variable.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import Summarizer\n",
                "llm:Summarizer = Summarizer.getSummarizer(llm_config, logging_level=logging_level)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if verbose:\n",
                "    print (\"Summarizing {} Content Nodes.\".format(len(list_of_uids)))\n",
                "\n",
                "for uid in list_of_uids:\n",
                "    content = kg.get_by_uid(uid)\n",
                "    prompt_text = llm_prompt.format(content=content.text).strip()\n",
                " \n",
                "    if verbose:\n",
                "        print (\"Summarizing {uid}: {title}\".format(uid=uid,title=content.title))\n",
                "        print (\"Prompt is: {}\".format(prompt_text))\n",
                "\n",
                "    summary = llm.summarize(prompt_text)\n",
                "    \n",
                "    kg.add_summary(text=summary,content=content)\n",
                "    \n",
                "    if verbose:\n",
                "        print(\"Summary reduced {reduction}% of content: {text}\\n\".format(reduction=round(((len(content.text)-len(summary))/len(content.text))*100,1),text=summary))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kg.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}