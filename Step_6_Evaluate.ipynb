{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 6 Evaluation\n",
                "\n",
                "If there are evaluation keys availible for the dataset, you can automatically run a set of evaluation metrics. These are generally\n",
                "geared to assessing the end-to-end result but obviously there is a relationship with internal steps as well.\n",
                "\n",
                "![step6](resources/Step6_Evaluate.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import opentldr\n",
                "from datetime import datetime"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "OpenTLDR workflows use the notebook block tagged as \"parameters\" to inject variables (for example which scores to produce).\n",
                "\n",
                "> **Do Not Change Variable Names in the Parameters Block** you are welcome to change the values of these parameter variables, but please do not change their names. They are used elsewhere in the notebook and in other workflow processes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Workflow Parameters\n",
                "eval_data_repo_config = {'repo_type': 'files', 'path': './sample_data/evalkey'}\n",
                "\n",
                "sentence_embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cosin Distance of text embeddings\n",
                "This is used to grossly estimate how similar two text blocks are to each other."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer, util\n",
                "model = SentenceTransformer(sentence_embedding_model)\n",
                "\n",
                "def cosin_similarity(string_1:str, string_2:str):\n",
                "        if string_1 == string_2:\n",
                "                return 1.0\n",
                "\n",
                "        #compute the embeddings for each string\n",
                "        embedding_1= model.encode(string_1, convert_to_tensor=True)\n",
                "        embedding_2 = model.encode(string_2, convert_to_tensor=True)\n",
                "        \n",
                "        #compute the cosin similarity of the two embeddings\n",
                "        similarity = util.cos_sim(embedding_1, embedding_2).cpu().numpy()[0][0]\n",
                "\n",
                "        return round(similarity,4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load the EvalKey entries from the data repository\n",
                "EvalKeys indcate correct answers like a rubric for the associated content/requests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from opentldr import KnowledgeGraph, DataRepo\n",
                "\n",
                "kg=KnowledgeGraph()\n",
                "\n",
                "# uncomment if you run this notebook repeatedly\n",
                "kg.delete_all_evalkeys()\n",
                "\n",
                "if eval_data_repo_config is not None:\n",
                "    repo = DataRepo(kg, eval_data_repo_config)\n",
                "    list_of_uids =  repo.importData()\n",
                "    print(\"Loaded {count} EvalKey nodes from the repository.\".format(count=len(list_of_uids)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Score each Content/Request Combination Pairwise\n",
                "Since these might be large text blocks in some cases, this is done by lists of the uids, and the loading of nodes is done on demand (this reduced memory requirements at the cost of more small but slow queries)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from opentldr.Domain import Content, Request, EvalKey, TldrEntry, Recommendation, Summary\n",
                "\n",
                "content_uids:list[str]=kg.get_all_node_uids_by_tag('Content')\n",
                "request_uids:list[str]=kg.get_all_node_uids_by_tag('Request')\n",
                "\n",
                "print(\"Evaluate pairwise {x} Content and {y} Request nodes.\".format(x=len(content_uids),y=len(request_uids)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## For each pair, compute the metrics\n",
                "We are interested in the average accross all pairs, so these are aggregate values until normalized later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tp:int = 0\n",
                "fp:int = 0\n",
                "tn:int = 0\n",
                "fn:int = 0\n",
                "\n",
                "selection:float = 0.0\n",
                "focus:float = 0.0\n",
                "reduction:float = 0.0\n",
                "reduction_count:int = 1\n",
                "\n",
                "content:Content = None\n",
                "for c in content_uids:\n",
                "\n",
                "    content = kg.get_content_by_uid(c)\n",
                "    #print()\n",
                "    #print (\"content:\\t\",content.to_text())\n",
                "    request:Request = None\n",
                "\n",
                "    for r in request_uids:\n",
                "\n",
                "        request= kg.get_request_by_uid(r)\n",
                "        #print(\"request:\\t\",request.to_text())\n",
                "\n",
                "        key:EvalKey = kg.cypher_query_one(\"\"\"\n",
                "            MATCH (c:Content) WHERE c.uid='{content_id}'\n",
                "            MATCH (q:Request) WHERE q.uid='{request_id}'\n",
                "            MATCH (c)<-[kc:KEY_FOR_CONTENT]-(k:EvalKey)-[kq:KEY_FOR_REQUEST]->(q)\n",
                "            RETURN k \"\"\".format( content_id=c, request_id=r),\"k\")\n",
                "\n",
                "        entry:TldrEntry = kg.cypher_query_one(\"\"\"\n",
                "            MATCH (c:Content) WHERE c.uid='{content_id}'\n",
                "            MATCH (q:Request) WHERE q.uid='{request_id}'\n",
                "            MATCH (q)<-[]-(tldr:Tldr)-[]->(e:TldrEntry)-[]->(r:Recommendation)-[]->(c)\n",
                "            RETURN e \"\"\".format( content_id=c, request_id=r),\"e\")\n",
                "\n",
                "        if entry is None:\n",
                "            if key is None:\n",
                "                # TRUE NEGATIVE\n",
                "                tn += 1\n",
                "                selection += 1.0\n",
                "                focus +=  1.0\n",
                "            else:\n",
                "                # FALSE NEGATIVE\n",
                "                #print (\"evalkey:\\t\",key.to_text())\n",
                "                fn += 1\n",
                "                selection += 1.0 - key.score\n",
                "                focus += 0.0\n",
                "        else:\n",
                "            #print (\"tldr:\\t\",entry.to_text())\n",
                "\n",
                "            recommendation:Recommendation = kg.get_recommendation(content,request)\n",
                "            #print (\"recommendation:\\t\",recommendation.to_text())\n",
                "\n",
                "            summary:Summary = kg.get_summaries_by_recommendation(recommendation)[0]\n",
                "            #print (\"summary:\\t\",summary.to_text())\n",
                "\n",
                "            if key is None:\n",
                "                # FALSE POSTIVE\n",
                "                fp += 1\n",
                "                selection += 1.0 - recommendation.score\n",
                "                focus += 1.0 - cosin_similarity(summary.text,request.text)\n",
                "                reduction += 1.0 - (len(summary.text) / len(content.text))\n",
                "                reduction_count += 1\n",
                "            else:\n",
                "                # TRUE POSITIVE\n",
                "                #print (\"evalkey:\\t\",key.to_text())\n",
                "                tp += 1\n",
                "                selection += 1.0 - abs(key.score - recommendation.score)\n",
                "                focus += cosin_similarity(key.text,summary.text)\n",
                "                reduction += 1.0 - (len(summary.text) / len(content.text))\n",
                "                reduction_count += 1\n",
                "\n",
                "total:int = tp+fp+tn+fn\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluation Results\n",
                "\n",
                "## Totals / Rates\n",
                "Confusion matrics of counts and relative rates of the pairwise (requests and content nodes) checks:\n",
                "- True Positives (tp): Content was included in the TLDR for a Request, AND an EvalKey exists for this pair.\n",
                "- False Positives (fp): Content was included in the TLDR for a Request, but there was no cooresponding EvalKey.\n",
                "- True Negative (tn): Content was skipped for this Request, AND there was no cooresponding EvalKey.\n",
                "- False Negative (fn): Content was skipped for this Request, but an EvalKey exists for this pair.\n",
                "\n",
                "## Metrics\n",
                "Computed assessments for end-to-end result:\n",
                "- Precision, Accuracy, Recall, and F1 Score: based on commonly used formula (see wikipedia) using above confusion matrix\n",
                "- Selection: The average amount that the recommendation scores were off of those provided in EvalKeys\n",
                "- Focus: The cosin similarity between the summary and the ideal (manually created) summary in the EvalKeys\n",
                "- Reduction: The % reduction in size (measured by characters) from the original content and the produced summary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Totals:\\n\",\n",
                "            \" n:\\t{x}\\n\".format(x=total),\n",
                "            \"tp:\\t{x}\\n\".format(x=tp),\n",
                "            \"tn:\\t{x}\\n\".format(x=tn),\n",
                "            \"fp:\\t{x}\\n\".format(x=fp),\n",
                "            \"fn:\\t{x}\\n\".format(x=fn))\n",
                "\n",
                "if total > 0.0:\n",
                "      print(\"Rates:\\n\",\n",
                "            \"tp:\\t{x:.3f} %\\n\".format(x=tp/total),\n",
                "            \"tn:\\t{x:.3f} %\\n\".format(x=tn/total),\n",
                "            \"fp:\\t{x:.3f} %\\n\".format(x=fp/total),\n",
                "            \"fn:\\t{x:.3f} %\\n\".format(x=fn/total))\n",
                "      \n",
                "      if (tp > 0.0):\n",
                "            precision:float = tp / (tp + fp)\n",
                "            accuracy:float = (tp + tn) / total\n",
                "            recall:float = tp / (tp + fn)\n",
                "            f1_score:float = tp / (tp + (0.5 * (fp + fn)))\n",
                "\n",
                "            reduction_avg:float = reduction / reduction_count\n",
                "            selection_avg:float = selection / total\n",
                "            focus_avg:float = focus / total\n",
                "\n",
                "            print(\"Metrics:\\n\",\n",
                "                  \"precision:\\t{x:.3f}\\n\".format(x=precision),\n",
                "                  \"accuracy: \\t{x:.3f}\\n\".format(x=accuracy),\n",
                "                  \"recall:   \\t{x:.3f}\\n\".format(x=recall),\n",
                "                  \"f1_score: \\t{x:.3f}\\n\".format(x=f1_score),\n",
                "                  \"selection:\\t{x:.3f}\\n\".format(x=selection_avg),\n",
                "                  \"focus:    \\t{x:.3f}\\n\".format(x=focus_avg),\n",
                "                  \"reduction:\\t{x:.3f}\\n\".format(x=reduction_avg)\n",
                "            )\n",
                "      else:\n",
                "            print(\"No true-positive results, skipping metrics.\")\n",
                "else:\n",
                "      print (\"No results.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kg.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}