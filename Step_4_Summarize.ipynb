{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Step 4: Summarize\n",
                "Uses the recommendations and related paths to focus an LLM summerization on the relevant information.\n",
                "\n",
                "![step4](resources/Step4_Summarize.png)\n",
                "\n",
                "The result of this step includes:\n",
                "- Summary nodes, connected to Content nodes with a SUMMARIZES relationship and to Recommendation nodes with a FOCUS_ON relationship"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "from opentldr import KnowledgeGraph\n",
                "kg=KnowledgeGraph()\n",
                "\n",
                "import opentldr.Domain as domain"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "OpenTLDR workflows use the notebook block tagged as \"parameters\" to inject variables (for example to change the LLM model).\n",
                "\n",
                "> **Do Not Change Variable Names in the Parameters Block** you are welcome to change the values of these parameter variables, but please do not change their names. They are used elsewhere in the notebook and in other workflow processes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "#Parameters\n",
                "\n",
                "# When run an LLM locally, you need to download the model to your local machine\n",
                "llm_model_path = \"./models/mistral-7b-openorca.gguf2.Q4_0.gguf\"\n",
                "\n",
                "llm_prompt = '''\n",
                "    Given these facts: {knowledge}\n",
                "    Write a concise summary of this content: {content}\n",
                "    Focus the summary to answer this request: {request}\n",
                "    CONCISE SUMMARY: '''\n",
                "\n",
                "llm_device = 'cpu'\n",
                "#llm_device = 'gpu'\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# uncomment if you run this notebook repeatedly\n",
                "#kg.delete_all_summaries()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify that a GPT4ALL Model is availible on the specified path.\n",
                "When you run an LLM localling using GPT4ALL, you need to download a model file to your local machine.\n",
                "\n",
                "- Model files are large and not part of the git repository.\n",
                "- You can download them from here: https://gpt4all.io under \"Model Explorer\" and put them in a \"models\" folder.\n",
                "- Be sure to check the license for the model before using.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists(llm_model_path):\n",
                "    raise ValueError (\"No LLM Model File was found at {path}\".format(path=llm_model_path))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Prepare the LLM for producing summerizations\n",
                "- TODO: in my case I run the LLM local but connecting to a remote ChatGPT might be interesting too"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chains.summarize import load_summarize_chain\n",
                "from langchain.llms import GPT4All\n",
                "from langchain import PromptTemplate, LLMChain\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.chains.mapreduce import MapReduceChain\n",
                "from langchain.prompts import PromptTemplate\n",
                "from langchain.docstore.document import Document\n",
                "from langchain.text_splitter import CharacterTextSplitter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## load the gpt4all model and build the prompt with lang chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm=GPT4All(model=llm_model_path, backend='gptj', device=llm_device, verbose=False)\n",
                "print(\"Using device: {device}\".format(device=llm.device))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### This is the default langchain summerization chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sum_chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
                "\n",
                "def summarize(content_text):\n",
                "    # put the content into a format that works for prompts\n",
                "    out=\"\"\n",
                "    text_splitter = CharacterTextSplitter()\n",
                "    texts = text_splitter.split_text(content_text)\n",
                "    docs = [Document(page_content=t) for t in texts[:3]]\n",
                "    if len(docs) > 0:\n",
                "        out= sum_chain.run(docs)\n",
                "    return out.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### This is a custom langchain summerization with focus chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "focused_chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
                "\n",
                "# an attempt at starting a customized prompt for summary\n",
                "def summarizeWithFocus(content_body,focus_text,info_request):\n",
                "    out=\"\"\n",
                "\n",
                "    # Assemble a prompt to summarize the article\n",
                "    all_text = llm_prompt.format(knowledge=focus_text, content=content_body, request=info_request).strip()\n",
                "\n",
                "    print(\"Prompt: \",all_text.strip())\n",
                "    text_splitter = CharacterTextSplitter()\n",
                "    texts = text_splitter.split_text(all_text)\n",
                "    docs = [Document(page_content=t) for t in texts[:3]]\n",
                "\n",
                "    # run the summarization chain on the combined prompt\n",
                "    if len(docs) > 0:\n",
                "        out= focused_chain.run(docs)\n",
                "    return out.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Display the shortest path between each request and content\n",
                "This next step does the same pairwise comparison but output sthe path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# produces a text string from a node or edge, if supported\n",
                "def explain(something):\n",
                "    if hasattr(something, 'to_text') and callable(something.to_text):\n",
                "        return something.to_text()\n",
                "    \n",
                "    return \"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate the shortest paths that provide a context for the prompt\n",
                "for request in kg.get_all_requests():\n",
                "    for content in kg.get_all_content():\n",
                "        path = kg.shortest_path(request,content)\n",
                "        if path is not None:\n",
                "            print(\"{score}\\t{req} -?-> {art}\".format(score=len(path),req=request.title,art=content.title))\n",
                "            for h in path:\n",
                "                print(\"\\t\",explain(h))\n",
                "            print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build the prompt and run the LLM\n",
                "This includes the original content, the request it is tailored for, and the explaination of the shortest path through the knowledge graph used to connect them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "requests = kg.get_all_requests()\n",
                "\n",
                "for request in requests:\n",
                "    print(\"Request ({request}):\".format(request=request.text))\n",
                "    recommendations = kg.get_recommendations_by_request(request=request)\n",
                "    \n",
                "    for recommendation in recommendations:\n",
                "        for content in recommendation.recommends:\n",
                "            print (\"Recommended {title} ({score}): {url}\".format(title=content.title, score=round(recommendation.score,3),url=content.url))\n",
                "            path_text=\"\"\n",
                "            path=kg.shortest_path(request,content)\n",
                "\n",
                "            if path is not None:\n",
                "                for hop in path:\n",
                "                    path_text+=explain(hop)+\" \"\n",
                "            \n",
                "            original= content.text\n",
                "            #print(\"\\tOriginal Content:\\t{text}\".format(text=original))\n",
                "            #print(\"\\tPath Text:\\t{text}\".format(text=path_text))\n",
                "\n",
                "            # Uncomment to see a summary without a path focus\n",
                "            #summary= summarize(content.text)\n",
                "            #print(\"summary ({reduction}):\\t{text}\".format(reduction=round(len(summary)/len(original),3),text=summary))\n",
                "\n",
                "            focused= summarizeWithFocus(content.text,str(path_text),request.text)\n",
                "            print(\"Summary reduced {reduction}% of content:\\t{text}\".format(reduction=round(((len(original)-len(focused))/len(original))*100,1),text=focused))\n",
                "\n",
                "            kg.add_summary(text=focused,content=content,recommendation=recommendation)\n",
                "            print(\"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kg.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}