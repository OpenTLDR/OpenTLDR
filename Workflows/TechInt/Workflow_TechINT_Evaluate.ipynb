{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "from datetime import date\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "from opentldr import Workflow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Specify the Workflow in code\n",
                "The workflow includes:\n",
                "- **Output**: the directory that the workflow writes copies of the notebooks as executed (read only!)\n",
                "- **Notebooks**: this is a list of notebooks (full path) in the order that they should be executed\n",
                "    - For each notebook the set of parameters that are to be passed into it thru the workflow process"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameters\n",
                "- use_local_files (boolean) - sets the data repo to either local (true) or S3 (false)\n",
                "- data_directory (path as string) - if using a local directory, which directory should be used.\n",
                "\n",
                "Note: regardless of where the content is pulled from, the reference data .csv files need to be in the directory specified by the data_directory parameter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Parameters\n",
                "\n",
                "# The main directory for loading data for this notebook\n",
                "use_local_files = True\n",
                "\n",
                "live = False\n",
                "today = str(date.today())\n",
                "\n",
                "data_directory = os.getenv(\"LIVE_DATA_REPO_PATH\", \"../Data/TechINT\")\n",
                "reference_data_path = os.getenv(\"REFERENCE_DATA_PATH\", \"../Data/TechINT/reference\")\n",
                "media_cache_path = os.getenv(\"MEDIA_CACHE_PATH\",\"../Data/TechINT/MediaCache\" )\n",
                "\n",
                "llm_device = \"cpu\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print (data_directory)\n",
                "print (reference_data_path)\n",
                "print (media_cache_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generate a Data Repo Config\n",
                "The get_data_repo_config(folder) function produces a data repo configure for either a local path or an s3 bucket, this allows parameters to change the data source without needing to edit the actual workflow content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_data_repo_config(folder:str, today:str = str(date.today())) -> dict:\n",
                "    '''\n",
                "    format the data_repo_config json based on above and given subfolder\n",
                "    '''\n",
                "    path = str(os.path.join(data_directory,folder))\n",
                "    \n",
                "    if live:\n",
                "        path = str(os.path.join(data_directory,today,folder))\n",
                "\n",
                "    if use_local_files:\n",
                "        return {\n",
                "            'repo_type': 'files',\n",
                "            'path': path\n",
                "        }\n",
                "    else:\n",
                "        return {\n",
                "            'repo_type': 's3',\n",
                "            'bucket': os.getenv(\"S3_BUCKET\"),\n",
                "            'aws_access_key_id': os.getenv(\"S3_ACCESS_KEY_ID\"),\n",
                "            'aws_secret_access_key': os.getenv(\"S3_SECRET_KEY\"),\n",
                "            'prefix': path\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define the Workflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "workflow = {\n",
                "\n",
                "    # Where a read-only version of the notebook AFTER execution is stored\n",
                "    \"Output\": \"../READ_ONLY_OUTPUT_TECHNINT\",\n",
                "  \n",
                "    # Parameters passed into all notebooks in workflow\n",
                "    \"Common\": {\n",
                "        \"logging_level\":logging.INFO,\n",
                "        \"verbose\": True,\n",
                "    },\n",
                "\n",
                "    # Order and parameters of notebooks to execute in workflow\n",
                "    \"Notebooks\": [\n",
                "\n",
                "        # Erases the entire KG - so only do this when you intend to\n",
                "        [\"../Stage_1_Initialize/Clear_All.ipynb\",{}],\n",
                "\n",
                "        # Load up a Reference Data KG that is relevent to the Arxiv dataset\n",
                "        [\"../Stage_1_Initialize/Load_CSV_Reference_Data.ipynb\",{\n",
                "            \"keyword_to_concept_csv_file\": os.path.join(reference_data_path,\"keywords.csv\"),\n",
                "            \"concept_to_concept_csv_file\": os.path.join(reference_data_path,\"concepts.csv\"),\n",
                "        }],\n",
                "\n",
                "        # Load up the set of example Requests (active data)\n",
                "        [\"../Stage_2_Ingest/Load_Requests.ipynb\",{\n",
                "            \"data_repo_config\": get_data_repo_config('request', today),\n",
                "        }],\n",
                "\n",
                "        # Load up the set of example Content (also active data)\n",
                "        [\"../Stage_2_Ingest/Load_Content.ipynb\",{\n",
                "            \"data_repo_config\": get_data_repo_config('content', today),\n",
                "        }],\n",
                "\n",
                "        # Load up the set of example Content (also active data)\n",
                "        [\"../Collectors/Enrich_Content.ipynb\",{\n",
                "            #\"content_uids\": [],\n",
                "            #\"request_uids\": [],\n",
                "            \"media_cache_path\": media_cache_path,\n",
                "            \"data_repo_config\": None,   # content was already ingested\n",
                "        }], \n",
                "\n",
                "        # Generate stand-alone untailored summaries ONLY NEEDED for multi-doc comparisons in digger\n",
                "        [ \"../Stage_5_Summarize/Presummarize.ipynb\",{\n",
                "            \"llm_config\" : {'type': 'GPT4ALL', 'device':llm_device, 'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'}\n",
                "        }],\n",
                "\n",
                "        # Make the connections between active data and reference data\n",
                "        [ \"../Stage_3_Connect/Entity_Cosin_Similarity.ipynb\",{\n",
                "            \"sentence_embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
                "            \"connect_threshold\": 0.25,\n",
                "            \"hypothesize_threshold\": 0.80\n",
                "        }],\n",
                "\n",
                "        # Find paths thru the KG and determine what should be recommended\n",
                "        [ \"../Stage_4_Recommend/Shortest_Path_Scoring.ipynb\", {\n",
                "            \"recommendation_threshold\": 0.75\n",
                "        }],\n",
                "\n",
                "        # Create summaries for content that has been recommended \n",
                "        [ \"../Stage_5_Summarize/Tailored_Abstractive_Summary.ipynb\",{\n",
                "            \"llm_config\" : {'type': 'GPT4ALL', 'device':llm_device, 'model':'../LLM_Models/mistral-7b-openorca.gguf2.Q4_0.gguf'},\n",
                "            \"llm_prompt\": \"Respond to the request: {request} while concisely summarizing this technical paper: {content}. Use the following facts: {knowledge}\",\n",
                "        }],\n",
                "\n",
                "        # Build this into a TLDR structure (basically Tldr and TldrEntries)\n",
                "        [\"../Stage_6_Produce/Build_TLDR.ipynb\",{\n",
                "        }],\n",
                "\n",
                "        # If there are any evalkeys for this data, apply them to see how it scores\n",
                "        [\"../Stage_7_Evaluate/Evaluate.ipynb\",{\n",
                "            \"data_repo_config\": get_data_repo_config('evalkey', today),\n",
                "            \"sentence_embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "        }],\n",
                "    ]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wf:Workflow = Workflow(workflow)\n",
                "wf.run()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}