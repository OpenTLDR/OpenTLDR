{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arxiv Collector for OpenTLDR (Update)\n",
                "This notebook pulls raw data from the internet and formats it for OpenTLDR ingest as a .json file.\n",
                "The output json file can be shared with multiple OpenTLDR instances as a file system or S3 bucket.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "import logging\n",
                "from datetime import datetime, date\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "from RssFeed import RssFeed\n",
                "from RepoWriter import RepoWriter\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "Parameters can be passed into the notebook using OpenTLDR Workflows for automation.\n",
                "The values set in the cell below are set as defaults for this collector, if not overridden by the Workflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Parameters\n",
                "\n",
                "# The path where this agent should write the .json files for content\n",
                "live_data_repo_path = os.getenv('LIVE_DATA_REPO_PATH')\n",
                "\n",
                "# Name of Source to use in OpenTLDR objects\n",
                "rss_source = \"arxiv\"\n",
                "organize_by_source = False\n",
                "\n",
                "# URLs for the feeds to scrape for this Source\n",
                "rss_feed_urls = [\n",
                "    \"http://rss.arxiv.org/rss/cs.AI\",\n",
                "    \"http://rss.arxiv.org/rss/cs.CR\",\n",
                "    \"http://rss.arxiv.org/rss/cs.AR\",\n",
                "    \"http://rss.arxiv.org/rss/cs.CV\",\n",
                "    \"http://rss.arxiv.org/rss/cs.CR\",\n",
                "    \"http://rss.arxiv.org/rss/cs.DL\",\n",
                "    \"http://rss.arxiv.org/rss/cs.ET\",\n",
                "    \"http://rss.arxiv.org/rss/cs.MA\",\n",
                "    \"http://rss.arxiv.org/rss/cs.NE\",\n",
                "    \"http://rss.arxiv.org/rss/cs.SC\",\n",
                "]\n",
                "\n",
                "# Some sites verify recent user_agent configurations\n",
                "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
                "\n",
                "download_media = True\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level = logging.INFO\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Process Arxiv RSS Feed\n",
                "This collector pulls entries from the RSS Feed and structures them for OpenTLDR Ingest.\n",
                "There are several Arxiv-specific clean-up and extra steps (e.g., download the referenced pdf file)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "reader=RssFeed(rss_source)\n",
                "\n",
                "# Determine how to write out data\n",
                "\n",
                "if live_data_repo_path is None:\n",
                "    live_data_repo_path = os.path.join(\"..\",\"Data\",\"Live\")\n",
                "\n",
                "write_path = os.path.join(live_data_repo_path)\n",
                "\n",
                "if organize_by_source:\n",
                "    write_path= os.path.join(live_data_repo_path,rss_source)\n",
                "\n",
                "print (\"Writing out json objects to: {}\".format(write_path))\n",
                "writer=RepoWriter(write_path)\n",
                "\n",
                "#reader.archive = os.path.join(live_data_repo_path,\"rss_archive\",datetime.now().strftime(\"%Y-%m-%d\"))\n",
                "\n",
                "# For Debugging\n",
                "reader.set_log_level(logging_level)\n",
                "writer.set_log_level(logging_level)\n",
                "\n",
                "for feed in rss_feed_urls:\n",
                "    for entry in reader.fetch_feed(feed, type='technical paper'):\n",
                "        \n",
                "        # Clean up source-specific issues\n",
                "        entry['text'] = entry['text'].split(\"\\nAbstract:\")[1].strip() \n",
                "\n",
                "        # Source-specific media collection\n",
                "        source_pdf_url = entry['url'].replace(\"abs\",\"pdf\")\n",
                "        entry['metadata']['full_content_pdf']= source_pdf_url\n",
                "\n",
                "        source_html_url = entry['url'].replace(\"abs\",\"html\")\n",
                "        entry['metadata']['full_content_html']= source_html_url\n",
                "\n",
                "        # Write out a content node \n",
                "        hash = writer.write_content(entry)\n",
                "        \n",
                "        # Some entries are skipped (hash == None), otherwise download Full PDF file in raw folder\n",
                "        if hash is not None and download_media:\n",
                "            try:\n",
                "                media_dir = os.path.join(write_path, entry['date'],\"media\")\n",
                "                pdf_media_path = os.path.join(media_dir,\"{}.pdf\".format(hash))\n",
                "                html_media_path = os.path.join(media_dir,\"{}.html\".format(hash))\n",
                "\n",
                "                if not os.path.exists(media_dir):\n",
                "                    os.makedirs(media_dir)\n",
                "\n",
                "                print (\"Downloading media files for {}\".format(hash))\n",
                "                writer.http_copy(source_pdf_url, pdf_media_path)\n",
                "                writer.http_copy(source_html_url, html_media_path)\n",
                "\n",
                "            except:\n",
                "                print (\"Failed to download PDF file for {}.\".format(hash))\n",
                "\n",
                "  "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "opentldr-collectors"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}