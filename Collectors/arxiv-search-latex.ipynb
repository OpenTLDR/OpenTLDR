{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Install bs4 and lxml before starting\n",
                "\n",
                "import urllib, urllib.request\n",
                "\n",
                "def run_search(search_term,max_results):\n",
                "    url = f'http://export.arxiv.org/api/query?search_query={search_term.replace(\" \", \"+\")}&sortBy=relevance&start=0&max_results={max_results}'\n",
                "    data = urllib.request.urlopen(url)\n",
                "    return(data.read().decode('utf-8'))\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "directory = \"./content/\"\n",
                "\n",
                "search_term = 'satellite IOT'\n",
                "max_results = '3'\n",
                "\n",
                "xml_data = run_search(search_term,max_results)\n",
                "print(xml_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import os\n",
                "\n",
                "def download_latex(content, directory):\n",
                "    uid = hash(content[\"url\"])\n",
                "    if uid < 0:\n",
                "        uid = \"{p}{n}\".format(p=\"x\",n=abs(uid))\n",
                "\n",
                "    filename = \"{uid}.tar.gz\".format(uid=uid)\n",
                "    fullpath = os.path.join(directory,filename)\n",
                "\n",
                "    # If hash(url) exists, then skip this\n",
                "    if os.path.exists(fullpath):\n",
                "        print(\"Skipping duplicate: {}\".format(fullpath))\n",
                "        return\n",
                "\n",
                "    latex_url = content[\"url\"].replace(\"abs\",\"src\")\n",
                "    latex_url = latex_url.replace(\"http://arxiv.org/\", \"http://export.arxiv.org/\")\n",
                "\n",
                "    # Get response object for link\n",
                "    response = requests.get(latex_url)\n",
                "\n",
                "    # Write content in pdf file\n",
                "    latex = open(fullpath, 'wb')\n",
                "    latex.write(response.content)\n",
                "    latex.close()\n",
                "    print(\"wrote: {}\".format(filename))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from bs4 import BeautifulSoup\n",
                "import json\n",
                "import os\n",
                "\n",
                "def fetch_search_resuls(xml_data, directory):\n",
                "\n",
                "    soup = BeautifulSoup(xml_data, \"lxml\")\n",
                "\n",
                "    # Extract the source\n",
                "    source = soup.find('feed').find('title').text.strip()\n",
                "\n",
                "    \n",
                "    entries = soup.find_all('entry')\n",
                "    for entry in entries:\n",
                "        # Extract the title of the entry\n",
                "        title = entry.find('title').text.strip()\n",
                "        \n",
                "        # Extract the publication date and format it as 'YYYY-MM-DD'\n",
                "        date = entry.find('published').text.strip()[:10]\n",
                "        \n",
                "        # Extract the primary category term\n",
                "        def is_primary_category_tag(tag):\n",
                "            return tag.name.endswith('primary_category')\n",
                "        primary_category = entry.find(is_primary_category_tag)\n",
                "        if primary_category:\n",
                "            type_term = primary_category.get('term')\n",
                "        else:\n",
                "            type_term = None\n",
                "        \n",
                "        # Extract the authors and join their names into a single string\n",
                "        authors = entry.find_all('author')\n",
                "        author_names = []\n",
                "        for author in authors:\n",
                "            name_elem = author.find('name')\n",
                "            if name_elem:\n",
                "                author_names.append(name_elem.text.strip())\n",
                "        author_str = ', '.join(author_names)\n",
                "        \n",
                "        # Extract the URL linking to the entry's HTML page\n",
                "        links = entry.find_all('link')\n",
                "        url = None\n",
                "        for link in links:\n",
                "            if link.get('rel') == 'alternate' and link.get('type') == 'text/html':\n",
                "                url = link.get('href')\n",
                "                break\n",
                "        if not url:\n",
                "            url = entry.find('id').text.strip()\n",
                "        \n",
                "        # Extract the summary text of the entry\n",
                "        text = entry.find('summary').text.strip()\n",
                "        \n",
                "        # Extract the DOI from the metadata\n",
                "        def is_doi_tag(tag):\n",
                "            return tag.name.endswith('doi')\n",
                "        arxiv_doi = entry.find(is_doi_tag)\n",
                "        if arxiv_doi:\n",
                "            doi = arxiv_doi.text.strip()\n",
                "        else:\n",
                "            doi = None\n",
                "        \n",
                "        # Extract the entry ID\n",
                "        entry_id = entry.find('id').text.strip()\n",
                "        \n",
                "        # Create a metadata dictionary including DOI and entry ID\n",
                "        metadata = {}\n",
                "        if doi:\n",
                "            metadata['doi'] = doi\n",
                "        if entry_id:\n",
                "            metadata['id'] = entry_id\n",
                "        \n",
                "        # Compile all extracted data into a single dictionary (almost the same as Rockfish)\n",
                "        # Note that Rockfish DOI used does not match the arxiv schema to form a valid doi.org link\n",
                "        content = {\n",
                "            'title': title,\n",
                "            'date': date,\n",
                "            'type': type_term,\n",
                "            'author': author_str,\n",
                "            'source': source,\n",
                "            'url': url,\n",
                "            'abstract': text,\n",
                "            'metadata': metadata\n",
                "        }\n",
                "        \n",
                "        # Generate a filename based on the url hash (matching Rockfish method)\n",
                "        uid = hash(content['url'])\n",
                "        if uid < 0:\n",
                "            uid = \"{p}{n}\".format(p=\"x\",n=abs(uid))\n",
                "\n",
                "        filename = \"{uid}.json\".format(uid=uid)\n",
                "        fullpath = os.path.join(directory,filename)\n",
                "        \n",
                "        # Write the data to a JSON file with indentation for readability\n",
                "        if os.path.exists(fullpath):\n",
                "            print(\"Skipping duplicate: {}\".format(fullpath))\n",
                "        else:\n",
                "            Export = {'Content' : [content]}  ## Match Rockfish json format.\n",
                "            with open(fullpath, 'w', encoding='utf-8') as json_file:\n",
                "                json.dump(Export, json_file, ensure_ascii=False, indent=4)\n",
                "            print(\"wrote: {}\".format(filename))\n",
                "        \n",
                "        download_latex(content, directory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fetch_search_resuls(xml_data, directory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Install python-magic before starting\n",
                "## Also requires latexml: sudo apt-get install latexml\n",
                "\n",
                "import tarfile\n",
                "import magic\n",
                "import os\n",
                "import subprocess\n",
                "from bs4 import BeautifulSoup\n",
                "import shutil\n",
                "\n",
                "\n",
                "def extract_arxiv_source(src_zip, dest_dir='./temp/'):\n",
                "    \n",
                "    # if 'gzip' not in magic.from_file(src_zip):\n",
                "    #     print(\"Not a gzip file\")\n",
                "    #     return None\n",
                "    \n",
                "    # dir = src_zip.replace(\".tar.gz\", \"\")\n",
                "    dir = src_zip.replace(\".tmp\", \"\")\n",
                "    fn = dir.split('/')[-1]\n",
                "    out_dir = os.path.join(dest_dir, fn)\n",
                "    if not os.path.isdir(out_dir):\n",
                "        os.mkdir(out_dir)\n",
                "    \n",
                "    try:  \n",
                "        with tarfile.open(src_zip) as tar:\n",
                "            tar.extractall(path=out_dir)\n",
                "    except Exception as e:\n",
                "        print(\"Error extracting file\")\n",
                "        print(str(e))\n",
                "        return None\n",
                "    \n",
                "    return out_dir\n",
                "\n",
                "def convert_arxiv_latex(latex_dir_in, html_dir_out):\n",
                "    tex_files = []\n",
                "    main_tex_file = None\n",
                "    \n",
                "    new_dir = latex_dir_in.split('/')[-1]\n",
                "    for file in os.listdir(latex_dir_in):\n",
                "        if file.endswith(\".tex\"):\n",
                "            tex_files.append(os.path.join(latex_dir_in, file))\n",
                "            \n",
                "    if len(tex_files) > 1:\n",
                "        main_tex_file = [f for f in tex_files if 'main' in tex_files]\n",
                "        # main_tex_file = [f if 'main' in tex_files else None for f in tex_files]\n",
                "        if len(main_tex_file) < 1:\n",
                "            for f in tex_files:\n",
                "                with open(f) as tex:\n",
                "                    if '\\\\title' in tex.read():\n",
                "                        main_tex_file = f\n",
                "                        print(f)\n",
                "                        break\n",
                "        else:\n",
                "            main_tex_file = main_tex_file[0]\n",
                "    else:\n",
                "        main_tex_file = tex_files[0]\n",
                "    \n",
                "    if main_tex_file is None:\n",
                "        print(\"Failed to find main tex file\")\n",
                "        return None\n",
                "    \n",
                "    output_file = os.path.join(html_dir_out, new_dir + \"-html/main.html\")\n",
                "    out = subprocess.run([\"latexmlc\", main_tex_file, \"--destination=\"+output_file], capture_output=True, text=True)\n",
                "    #print(out.stderr)\n",
                "    if out.returncode != 0:\n",
                "        # print(\"Failed to convert to html\")\n",
                "        # print(\"IMPORTANT! If you get an error running latexml, you may need to enable read/write for some ImageMagick policies\")\n",
                "        # print(\"Check by running...\")\n",
                "        # print(\"cat /etc/ImageMagic-6/policy.xml | grep PS\")\n",
                "        # print(\"Edit the policy file and change from rights='none' to rights='read|write' for pattern='PS' and pattern='EPS'\")\n",
                "        # print(out.stderr)\n",
                "        return None\n",
                "    # subprocess.run([\"latexmlc\", d + \"/\" + main_tex_file, \"--destination=\"+d+\"-html/main.html\"])\n",
                "    print(\"\\n\\n\")\n",
                "    return output_file\n",
                "\n",
                "def convert_html_text(html_file_in):\n",
                "    with open(html_file_in, 'r', encoding='utf-8') as file:\n",
                "        html_content = file.read()\n",
                "\n",
                "    soup = BeautifulSoup(html_content, 'html.parser')\n",
                "    plain_text = soup.get_text()\n",
                "\n",
                "    return plain_text\n",
                "\n",
                "def convert_cleanup(src_zip, dest_dir):\n",
                "    fn = src_zip.split('/')[-1]\n",
                "    directory_path = os.path.join(dest_dir, fn)\n",
                "    directory_path_html = directory_path + \"-html\"\n",
                "\n",
                "    # Check if the directory exists\n",
                "    if not os.path.exists(directory_path):\n",
                "        print(f\"The directory {directory_path} does not exist.\")\n",
                "    \n",
                "    if not os.path.exists(directory_path_html):\n",
                "        print(f\"The directory {directory_path_html} does not exist.\")\n",
                "    \n",
                "    # Prompt user with Yes/No confirmation\n",
                "    # confirm = input(f\"Are you sure you want to delete the directory '{directory_path}' and '{directory_path_html}' and all its contents? (yes/no): \").lower()\n",
                "    confirm = 'y'\n",
                "\n",
                "    if confirm in ['y', 'yes']:\n",
                "        try:\n",
                "            # Remove the entire directory and its contents\n",
                "            shutil.rmtree(directory_path)\n",
                "            print(f\"Directory '{directory_path}' and all its contents have been deleted.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to delete directory '{directory_path}'. Reason: {e}\")\n",
                "        \n",
                "        try:\n",
                "            # Remove the entire directory and its contents\n",
                "            shutil.rmtree(directory_path_html)\n",
                "            print(f\"Directory '{directory_path_html}' and all its contents have been deleted.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to delete directory '{directory_path_html}'. Reason: {e}\")\n",
                "\n",
                "    else:\n",
                "        print(\"Deletion canceled.\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "\n",
                "\n",
                "# Run all of the functions needed to create the plain text\n",
                "def text_from_latex_tar_gz(src_zip,temp_dir):\n",
                "    print(src_zip)\n",
                "    extracted_latex_dir = extract_arxiv_source(src_zip, temp_dir)\n",
                "    try: \n",
                "        html_output_file = convert_arxiv_latex(extracted_latex_dir, temp_dir)\n",
                "        print(\"Converstion from LaTex to HTML Complete\")\n",
                "    except:\n",
                "        print(\"Failed to convert LaTeX to HTML\")\n",
                "    \n",
                "    try: \n",
                "        plain_text = convert_html_text(html_output_file)\n",
                "        print(\"Converstion from HTML to text Complete\")\n",
                "\n",
                "    except:\n",
                "        print(\"Failed to convert HTML to text\")\n",
                "        plain_text = None\n",
                "\n",
                "    convert_cleanup(src_zip, temp_dir)\n",
                "\n",
                "    return plain_text\n",
                "\n",
                "#Find files in a directory by extention\n",
                "def find_files(directory, ext):\n",
                "    if not os.path.isdir(directory):\n",
                "        print(f\"Error: The directory '{directory}' does not exist.\")\n",
                "        return []\n",
                "\n",
                "    files = glob.glob(os.path.join(directory, '*.' + ext))\n",
                "\n",
                "    return files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Append data into an existing JSON file\n",
                "def append_text_to_arxiv_json(file_name, data_to_append):\n",
                "    try:\n",
                "        with open(file_name, 'r') as file:\n",
                "            json_data = json.load(file)\n",
                "        if 'Content' in json_data:\n",
                "            json_data['Content'][0].update(data_to_append)\n",
                "        else:\n",
                "            # If 'Content' does not exist, create it and add the data\n",
                "            json_data['Content'] = [data_to_append]\n",
                "\n",
                "        with open(file_name, 'w') as file:\n",
                "            json.dump(json_data, file, ensure_ascii=False, indent=4)\n",
                "\n",
                "        print(f\"Successfully appended to the JSON file: {file_name}\")\n",
                "\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: The file '{file_name}' was not found.\")\n",
                "    except json.JSONDecodeError:\n",
                "        print(f\"Error: The file '{file_name}' is not a valid JSON file.\")\n",
                "    except Exception as e:\n",
                "        print(f\"An unexpected error occurred: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp_dir = \"./temp/\"\n",
                "\n",
                "json_files = find_files(directory,'json')\n",
                "\n",
                "updated_files = []\n",
                "for json_file in json_files:\n",
                "    tar_gz_file = json_file.replace(\".json\", \".tar.gz\")\n",
                "    \n",
                "    plain_text = text_from_latex_tar_gz(tar_gz_file, temp_dir)\n",
                "    \n",
                "    if plain_text:\n",
                "        data_to_append = {'text': plain_text}\n",
                "        append_text_to_arxiv_json(json_file, data_to_append)\n",
                "        updated_files.append(json_file)\n",
                "    else:\n",
                "        print(f\"Error: No plain text returned from conversion. JSON file not updated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "updated_files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_name = './content/6847810983221028824.json'\n",
                "with open(file_name, 'r') as file:\n",
                "    json_data = json.load(file)\n",
                "\n",
                "json_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}