{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NPR Collector for OpenTLDR (Update)\n",
                "This notebook pulls raw data from the internet and formats it for OpenTLDR ingest as a .json file.\n",
                "The output json file can be shared with multiple OpenTLDR instances as a file system or S3 bucket.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "import logging\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "#from RssFeed import RssFeed\n",
                "from RepoWriter import RepoWriter\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "Parameters can be passed into the notebook using OpenTLDR Workflows for automation.\n",
                "The values set in the cell below are set as defaults for this collector, if not overridden by the Workflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Parameters\n",
                "\n",
                "# The path where this agent should write the .json files for content\n",
                "live_data_repo_path = os.getenv('LIVE_DATA_REPO_PATH')\n",
                "\n",
                "# The path where this agent should write full source files temporarily\n",
                "download_media = False          # NPR already loads full text by default\n",
                "media_cache_path = os.getenv('MEDIA_CACHE_PATH')\n",
                "\n",
                "# Name of Source to use in OpenTLDR objects\n",
                "source = \"npr\"\n",
                "organize_by_source = True\n",
                "\n",
                "source_url = \"https://text.npr.org\"\n",
                "\n",
                "pages_urls = [\n",
                "    \"https://text.npr.org\",         # Main Page\n",
                "    \"https://text.npr.org/1001\",    # News\n",
                "    \"https://text.npr.org/1008\",    # Culture\n",
                "    \"https://text.npr.org/1039\",    # Music\n",
                "]\n",
                "\n",
                "# Some sites verify recent user_agent configurations\n",
                "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
                "\n",
                "# Filters - excludes some content\n",
                "date:str = None\n",
                "days_history = 9999\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level = logging.INFO\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Parameters\n",
                "Parameters are passed in as strings that sometimes need to be converted to other object format.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the Data Filtering\n",
                "\n",
                "from datetime import datetime, timedelta\n",
                "import pytz\n",
                "\n",
                "# Default to today\n",
                "today = datetime.now()\n",
                "if date is not None: \n",
                "    today = datetime.strptime(date,\"%Y-%m-%d\")\n",
                "\n",
                "# defaults to previous 1 day\n",
                "last_read = today - timedelta(days=days_history)\n",
                "\n",
                "print(\"Today is {}, will atttempt to download content since {}.\".format(today.strftime('%Y-%m-%d'),last_read.strftime('%Y-%m-%d')))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify where the .json files should be stored.\n",
                "if live_data_repo_path is None:\n",
                "    live_data_repo_path = os.path.join(\"..\",\"Data\",\"Live\")\n",
                "\n",
                "if organize_by_source:\n",
                "    live_data_repo_path= os.path.join(live_data_repo_path,source)\n",
                "\n",
                "print (\"Writing Content to: {}\".format(live_data_repo_path))\n",
                "\n",
                "# Verify where the media files should be stored.\n",
                "if media_cache_path is None and download_media:\n",
                "    media_cache_path = os.path.join('..','Data','MediaCache')\n",
                "\n",
                "if download_media:\n",
                "    print (\"Cacheing Media files to: {}\".format(media_cache_path))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure that these paths exist and create them if they do not\n",
                "\n",
                "if not os.path.exists(live_data_repo_path):\n",
                "    print (\"Creating directory: {}\".format(live_data_repo_path))\n",
                "    os.makedirs(live_data_repo_path)\n",
                "\n",
                "if download_media and not os.path.exists(media_cache_path):\n",
                "    print (\"Creating directory: {}\".format(media_cache_path))\n",
                "    os.makedirs(media_cache_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Code for pulling and processing html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from datetime import datetime\n",
                "\n",
                "def fetch_html(url:str) -> str:\n",
                "    request=requests.get(url)\n",
                "    return request.content\n",
                "\n",
                "def process_text(in_html:str):\n",
                "    soup = BeautifulSoup(in_html,\"html.parser\")\n",
                "\n",
                "    author = \"unknown\"\n",
                "    date = datetime.now()\n",
                "\n",
                "    title_block = soup.find('h1', attrs={\"class\":\"story-title\"}, string=True)\n",
                "    if title_block is not None:\n",
                "        title =title_block.get_text()\n",
                "        #print (\"Got title: {}\".format(title))\n",
                "\n",
                "    else:\n",
                "        title = None\n",
                "\n",
                "    header_div = soup.find('div', attrs={\"class\":\"story-head\"}).find_all('p', string=True)\n",
                "    \n",
                "    for header_part in header_div:\n",
                "        line = header_part.get_text()\n",
                "        if line.startswith(\"By \"):\n",
                "            author = line[3:]\n",
                "            #print (\"Got author: {}\".format(author))\n",
                "\n",
                "        else:\n",
                "            try:\n",
                "                start_date_index = line.index(\", \")+2\n",
                "                end_date_index = line.index(\"\u2022\")-1\n",
                "                date_string = line[start_date_index : end_date_index]\n",
                "                date = datetime.strptime(date_string, \"%B %d, %Y\")\n",
                "                #print (\"Got date: {}\".format(date.strftime(\"%Y-%m-%d\")))\n",
                "            except:\n",
                "                #print (\"Not a date? >{}<\".format(line))\n",
                "                pass\n",
                "\n",
                "    npr_text_divs = soup.find('div', attrs={\"class\":\"paragraphs-container\"})\n",
                "    all_text = npr_text_divs.find_all('p', string=True)\n",
                "    output = ''\n",
                "\n",
                "    for text_part in all_text:\n",
                "        output = '{} \\n {}'.format(output, text_part.get_text())\n",
                "\n",
                "    #print (\"Got text: {}\".format(output))\n",
                "\n",
                "    return title, author, date.strftime('%Y-%m-%d'), output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NPR Text Website Processing\n",
                "This site has several top level pages with a list of articles, each is a link to an actual article.\n",
                "This code block reads the pages, iterates the list, and reads the each article.\n",
                "Content nodes are created for each article."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "writer=RepoWriter(live_data_repo_path)\n",
                "writer.set_log_level(logging_level)\n",
                "\n",
                "for page in pages_urls: \n",
                "    try:\n",
                "        # Download Topic Page with links to specific articles\n",
                "        main_page=fetch_html(page)\n",
                "        soup = BeautifulSoup(main_page,\"html.parser\")\n",
                "        date_html = soup.find('p', attrs={'class':'topic-date'})\n",
                "\n",
                "        # Site appears to have 2 day delay of articles\n",
                "        entry_date= datetime.strptime(date_html.text, \"%A, %B %d, %Y\")         \n",
                "        #entry_date = datetime.now()\n",
                "\n",
                "        # Iterate the list of entries (links to articles)\n",
                "        for article in soup.find_all('a', href=True, attrs={'class':'topic-title'}):\n",
                "            \n",
                "            try:\n",
                "                # Extract Content information\n",
                "                expected_title = article.text\n",
                "                link = \"{main}{rel}\".format(main=source_url, rel=article[\"href\"])\n",
                "\n",
                "                if verbose:\n",
                "                    print(\"Fetching article: {}\".format(link))\n",
                "\n",
                "                source_html = fetch_html(link)\n",
                "                title, author, article_date, text = process_text(source_html)\n",
                "\n",
                "                # sometimes the article format is odd\n",
                "                if title is None:\n",
                "                    title=expected_title\n",
                "\n",
                "                if article_date is None:\n",
                "                    article_date = entry_date.strftime(\"%Y-%m-%d\")               \n",
                "\n",
                "                entry:dict = {\n",
                "                    \"title\": title,\n",
                "                    \"date\": article_date,\n",
                "                    \"type\": \"news\",\n",
                "                    \"author\": author,\n",
                "                    \"source\": source,\n",
                "                    \"url\": link,\n",
                "                    \"text\": text,\n",
                "                    \"metadata\": {},\n",
                "                }\n",
                "\n",
                "                # Write out a content node only if its in the date range we want\n",
                "                article_date_obj = datetime.strptime(article_date,\"%Y-%m-%d\")\n",
                "                \n",
                "                repo_uid = None\n",
                "                if article_date_obj > last_read and article_date_obj <= today:\n",
                "                    repo_uid = writer.write_content(entry)\n",
                "\n",
                "                # Some entries are skipped (hash == None), otherwise store the html and any media found\n",
                "                # Note NPR uses text parsing of page - so this source html file is already downloaded at this point.\n",
                "                if repo_uid is not None and download_media:\n",
                "                    try:\n",
                "                        if not os.path.exists(media_cache_path):\n",
                "                            os.makedirs(media_cache_path)\n",
                "\n",
                "                        html_media_path = os.path.join(media_cache_path,\"{}.html\".format(repo_uid))\n",
                "\n",
                "                        with open(html_media_path,\"wb\") as f:\n",
                "                            f.write(source_html)\n",
                "                    except Exception as e:\n",
                "                        print (\"Failed to download media files for {}: {}\".format(repo_uid, e))\n",
                "            except Exception as e:\n",
                "                print (\"Failed to process article for {}: {}\".format(link, e))    \n",
                "            \n",
                "    except Exception as e:\n",
                "        print (\"Page {} had error: {}\".format(page, e))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}