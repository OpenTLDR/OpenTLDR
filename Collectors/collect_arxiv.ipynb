{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arxiv Collector for OpenTLDR (Update)\n",
                "This notebook pulls raw data from the internet and formats it for OpenTLDR ingest as a .json file.\n",
                "The output json file can be shared with multiple OpenTLDR instances as a file system or S3 bucket.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "import logging\n",
                "from datetime import datetime, date\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "from RssFeed import RssFeed\n",
                "from RepoWriter import RepoWriter\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "Parameters can be passed into the notebook using OpenTLDR Workflows for automation.\n",
                "The values set in the cell below are set as defaults for this collector, if not overridden by the Workflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Parameters\n",
                "\n",
                "# The path where this agent should write the .json files for content\n",
                "live_data_repo_path = os.getenv('LIVE_DATA_REPO_PATH')\n",
                "\n",
                "# The path where this agent should write full source files temporarily\n",
                "download_media = False\n",
                "media_cache_path = os.getenv('MEDIA_CACHE_PATH')\n",
                "arxiv_file_format = \"pdf\"\n",
                "\n",
                "# Name of Source to use in OpenTLDR objects\n",
                "rss_source = \"arxiv\"\n",
                "organize_by_source = True\n",
                "\n",
                "# URLs for the feeds to scrape for this Source\n",
                "rss_feed_urls = [\n",
                "#    \"http://rss.arxiv.org/rss/cs.AI\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.CR\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.AR\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.CV\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.CR\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.DL\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.ET\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.MA\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.NE\",\n",
                "#    \"http://rss.arxiv.org/rss/cs.SC\",\n",
                "]\n",
                "\n",
                "search_terms = [\n",
                "    \"Recent regulatory initiatives like the European AI Act and relevant voices in the Machine Learning (ML) community\",\n",
                "    \"The Role of Governments in Increasing Interconnected Post-Deployment Monitoring of AI\",\n",
                "    \"Voice-Enabled AI Agents can Perform Common Scams\",\n",
                "]\n",
                "\n",
                "# Filters - excludes some content\n",
                "max_search_results = 10\n",
                "date:str = None\n",
                "days_history = 9999\n",
                "\n",
                "# Some sites verify recent user_agent configurations\n",
                "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
                "\n",
                "# Logging level ranges are (from least to most verbose): ERROR, WARN, INFO, DEBUG\n",
                "logging_level = logging.INFO\n",
                "\n",
                "# level of unnecessary output\n",
                "verbose = True\n",
                "\n",
                "\n",
                "#TEMP\n",
                "rss_source=\"cyber\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Parameters\n",
                "Parameters are passed in as strings that sometimes need to be converted to other object format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the Data Filtering\n",
                "# Note - this is mostly useful for search term based filtering\n",
                "\n",
                "from datetime import datetime, timedelta\n",
                "import pytz\n",
                "\n",
                "# Default to today\n",
                "today = datetime.now()\n",
                "if date is not None: \n",
                "    today = datetime.strptime(date,\"%Y-%m-%d\")\n",
                "\n",
                "# defaults to previous 1 day\n",
                "last_read = today - timedelta(days=days_history)\n",
                "\n",
                "print(\"Today is {}, will atttempt to download content since {}.\".format(today.strftime('%Y-%m-%d'),last_read.strftime('%Y-%m-%d')))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add search terms to rss_feeds list\n",
                "\n",
                "for search_term in search_terms:\n",
                "    rss_feed_urls.append(f'http://export.arxiv.org/api/query?search_query={search_term.replace(\" \", \"+\")}&sortBy=relevance&start=0&max_results={max_search_results}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify where the .json files should be stored.\n",
                "if live_data_repo_path is None:\n",
                "    live_data_repo_path = os.path.join(\"..\",\"Data\",\"Live\")\n",
                "\n",
                "if organize_by_source:\n",
                "    live_data_repo_path= os.path.join(live_data_repo_path,rss_source)\n",
                "\n",
                "print (\"Writing Content to: {}\".format(live_data_repo_path))\n",
                "\n",
                "# Verify where the media files should be stored.\n",
                "if media_cache_path is None and download_media:\n",
                "    media_cache_path = os.path.join('..','Data','MediaCache')\n",
                "\n",
                "if download_media:\n",
                "    print (\"Cacheing Media files to: {}\".format(media_cache_path))\n",
                "    print (\"Downloading {} formated files.\".format(arxiv_file_format))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure that these paths exist and create them if they do not\n",
                "\n",
                "if not os.path.exists(live_data_repo_path):\n",
                "    print (\"Creating directory: {}\".format(live_data_repo_path))\n",
                "    os.makedirs(live_data_repo_path)\n",
                "\n",
                "if download_media and not os.path.exists(media_cache_path):\n",
                "    print (\"Creating directory: {}\".format(media_cache_path))\n",
                "    os.makedirs(media_cache_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Process Arxiv RSS Feed\n",
                "This collector pulls entries from the RSS Feed and structures them for OpenTLDR Ingest.\n",
                "There are several Arxiv-specific clean-up and extra steps (e.g., download the referenced pdf file)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "reader=RssFeed(rss_source, user_agent=user_agent)\n",
                "reader.set_filter(today=today, earliest=last_read)\n",
                "reader.set_log_level(logging_level)\n",
                "reader.archive = os.path.join(live_data_repo_path,\"rss_archive\",datetime.now().strftime(\"%Y-%m-%d\"))\n",
                "\n",
                "writer=RepoWriter(live_data_repo_path)\n",
                "writer.set_log_level(logging_level)\n",
                "\n",
                "for feed in rss_feed_urls:\n",
                "    try:\n",
                "        for entry in reader.fetch_feed(feed, type='technical paper'):\n",
                "            \n",
                "            # Clean up source-specific issues\n",
                "            if entry['text'].startswith(\"\\nAbstract:\"):\n",
                "                entry['text'] = entry['text'].split(\"\\nAbstract:\")[1].strip() \n",
                "\n",
                "            # Source-specific media collection - we record them all but only fetch what is required\n",
                "            source_pdf_url = entry['url'].replace(\"abs\",\"pdf\")\n",
                "            entry['metadata']['full_content_pdf']= source_pdf_url\n",
                "\n",
                "            source_html_url = entry['url'].replace(\"abs\",\"html\")\n",
                "            entry['metadata']['full_content_html']= source_html_url\n",
                "\n",
                "            source_latex_url = entry['url'].replace(\"abs\",\"src\")\n",
                "            entry['metadata']['full_content_latex']= source_latex_url\n",
                "\n",
                "            # Write out a content node \n",
                "            repo_uid = writer.write_content(entry)\n",
                "            \n",
                "            # Some entries are skipped (repo_uid == None), otherwise download and cache the document itself\n",
                "            if repo_uid is not None and download_media:\n",
                "                try:\n",
                "                    match arxiv_file_format:\n",
                "                        case 'pdf':\n",
                "                            if source_pdf_url is not None:\n",
                "                                pdf_media_path = os.path.join(media_cache_path,\"{}.pdf\".format(repo_uid))\n",
                "                                writer.http_copy(source_pdf_url, pdf_media_path)\n",
                "\n",
                "                        case 'html':\n",
                "                            if source_html_url is not None:\n",
                "                                html_media_path = os.path.join(media_cache_path,\"{}.html\".format(repo_uid))\n",
                "                                writer.http_copy(source_html_url, html_media_path)\n",
                "\n",
                "                        case 'latex':\n",
                "                            if source_latex_url is not None:\n",
                "                                latex_media_path = os.path.join(media_cache_path,\"{}.html\".format(repo_uid))\n",
                "                                writer.http_copy(source_latex_url, latex_media_path)\n",
                "\n",
                "                    if verbose:\n",
                "                        print (\"Downloaded media {} files for {}\".format(arxiv_file_format, repo_uid))\n",
                "\n",
                "                except Exception as e:\n",
                "                    print (\"Failed to download media {} file for {}: {}\".format(arxiv_file_format, repo_uid, e))\n",
                "    \n",
                "    except Exception as e:\n",
                "        print (\"RSS Feed {} had error: {}\".format(feed, e))\n",
                "    "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "opentldr-env",
            "language": "python",
            "name": "opentldr-collectors"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}